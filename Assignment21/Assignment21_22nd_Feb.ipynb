{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lightweight-franklin",
   "metadata": {},
   "source": [
    "## <u> Assignment 21 - 22nd Feb</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-madonna",
   "metadata": {},
   "source": [
    "#### URL To Scrape : https://www.youtube.com/@PW-Foundation/videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-camping",
   "metadata": {},
   "source": [
    "#### First, let's create base to extract youtube channel data. Here I am using json and regular expression library to parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "golden-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries for scraping\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cordless-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_base_url = 'https://www.youtube.com/watch?v='\n",
    "\n",
    "channel_url = 'https://www.youtube.com/@PW-Foundation/videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reduced-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "header={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "capable-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = req.get(channel_url, headers=header, cookies={'CONSENT':'YES+1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sensitive-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indie-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = re.search(r\"var ytInitialData = ({.*});\", str(soup.prettify())).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comprehensive-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.loads(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sealed-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tab 0 - Home\n",
    "tab 1 - Videos\n",
    "tab 2 - Shorts\n",
    "tab 3 - Live\n",
    "tab 4 - Playlist\n",
    "tab 5 - Community\n",
    "tab 6 - Channels\n",
    "tab 7 - About\n",
    "tab 8 - Search\n",
    "\n",
    "For this assignment, we are interested in tab 1 - Videos.\n",
    "\n",
    "\"\"\"\n",
    "tabs = json_data['contents']['twoColumnBrowseResultsRenderer']['tabs'][1]\n",
    "tabs = tabs['tabRenderer']['content']['richGridRenderer']['contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interstate-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_list = []\n",
    "for tab in tabs[:5]:\n",
    "    vid = tab['richItemRenderer']['content']['videoRenderer']\n",
    "    videos_list.append(vid)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-landing",
   "metadata": {},
   "source": [
    "#### Now, the `videos_list` object has first 5 videos details from the given channel. The data are in `json` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-round",
   "metadata": {},
   "source": [
    "#### Q1. Write a python program to extract the video URL of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chemical-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_get_video_url(lst):\n",
    "    \"\"\" Fetching videoId \"\"\"\n",
    "    vid_urls = []\n",
    "    for vid in lst:\n",
    "        link = yt_base_url + vid['videoId']\n",
    "        vid_urls.append(link)\n",
    "    \n",
    "    return vid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "together-november",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=46CNHP9wBAs',\n",
       " 'https://www.youtube.com/watch?v=1Wk5gJtZ2sQ',\n",
       " 'https://www.youtube.com/watch?v=iM_hVnElC-Q',\n",
       " 'https://www.youtube.com/watch?v=Prly9d7LoAQ',\n",
       " 'https://www.youtube.com/watch?v=AvwBDiCWSEM']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5urls = yt_get_video_url(videos_list)\n",
    "top5urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-enlargement",
   "metadata": {},
   "source": [
    "#### Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accompanied-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_get_video_thumbnail(lst):\n",
    "    \"\"\" Fetching video thumbnail \"\"\"\n",
    "    vid_thumbs = []\n",
    "    for vid in lst:\n",
    "        thumb = vid['thumbnail']['thumbnails'][-1]['url'].split('?')[0]\n",
    "        vid_thumbs.append(thumb)\n",
    "    \n",
    "    return vid_thumbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "developing-cinema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://i.ytimg.com/vi/46CNHP9wBAs/hqdefault.jpg',\n",
       " 'https://i.ytimg.com/vi/1Wk5gJtZ2sQ/hqdefault.jpg',\n",
       " 'https://i.ytimg.com/vi/iM_hVnElC-Q/hqdefault.jpg',\n",
       " 'https://i.ytimg.com/vi/Prly9d7LoAQ/hqdefault.jpg',\n",
       " 'https://i.ytimg.com/vi/AvwBDiCWSEM/hqdefault.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5thumbnails = yt_get_video_thumbnail(videos_list)\n",
    "top5thumbnails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-secondary",
   "metadata": {},
   "source": [
    "#### Q3. Write a python program to extract the title of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sensitive-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_get_video_title(lst):\n",
    "    \"\"\" Fetching video title \"\"\"\n",
    "    vid_titles = []\n",
    "    for vid in lst:\n",
    "        title = vid['title']['runs'][0]['text']\n",
    "        vid_titles.append(title)\n",
    "    \n",
    "    return vid_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accredited-password",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Complete ğ—”ğ—–ğ—œğ——, ğ—•ğ—”ğ—¦ğ—˜ğ—¦ ğ—”ğ—¡ğ—— ğ—¦ğ—”ğ—Ÿğ—§ in 110 Minutes | Class 10th Board Exam',\n",
       " 'Complete ğ—–ğ—›ğ—˜ğ— ğ—œğ—–ğ—”ğ—Ÿ ğ—¥ğ—˜ğ—”ğ—–ğ—§ğ—œğ—¢ğ—¡  in 90 Minutes | Class 10th Board Exam',\n",
       " 'Complete ğ‡ğ„ğ‘ğ„ğƒğˆğ“ğ˜ ğ€ğğƒ ğ„ğ•ğğ‹ğ”ğ“ğˆğğ in 2 Hours|| Class 10th Board Exam',\n",
       " 'Complete ğ—›ğ—¢ğ—ª ğ——ğ—¢ ğ—¢ğ—¥ğ—šğ—”ğ—¡ğ—œğ—¦ğ— ğ—¦ ğ—¥ğ—˜ğ—£ğ—¥ğ—¢ğ——ğ—¨ğ—–ğ—˜ in 1 Hours 50 Minutes|| Class 10th Board Exam',\n",
       " 'Complete ğ—¢ğ—¨ğ—¥ ğ—˜ğ—¡ğ—©ğ—œğ—¥ğ—¢ğ—¡ğ— ğ—˜ğ—¡ğ—§ in 1 Hour 30 Minutes | Class 10th Board Exam']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5titles = yt_get_video_title(videos_list)\n",
    "top5titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-publisher",
   "metadata": {},
   "source": [
    "#### Q4. Write a python program to extract the number of views of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "verbal-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_get_video_views(lst):\n",
    "    \"\"\" Fetching total number of views of video \"\"\"\n",
    "    vid_views = []\n",
    "    for vid in lst:\n",
    "        view = vid['viewCountText']['simpleText']\n",
    "        vid_views.append(view)\n",
    "    \n",
    "    return vid_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "satellite-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4,866 views', '14,646 views', '38,616 views', '17,563 views', '32,307 views']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5views = yt_get_video_views(videos_list)\n",
    "top5views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-participant",
   "metadata": {},
   "source": [
    "#### Q5. Write a python program to extract the time of posting of video for the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vanilla-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_get_video_post(lst):\n",
    "    \"\"\" Fetching the time of video publish on channel \"\"\"\n",
    "    vid_post = []\n",
    "    for vid in lst:\n",
    "        post = vid['publishedTimeText']['simpleText']\n",
    "        vid_post.append(post)\n",
    "    \n",
    "    return vid_post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "military-sculpture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 hours ago', '5 hours ago', '17 hours ago', '20 hours ago', '22 hours ago']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5post= yt_get_video_post(videos_list)\n",
    "top5post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reported-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_get_video_publish_date(lst):\n",
    "    \"\"\" \n",
    "    This function convert the absolute time into relative time.\n",
    "    It will show the actual date of video uploaded.\n",
    "    \"\"\"\n",
    "    from datetime import datetime as dt\n",
    "    from datetime import timedelta as td\n",
    "\n",
    "    today = dt.now()\n",
    "    date_lst = []\n",
    "    \n",
    "    for p in lst:\n",
    "        upload_dt = today\n",
    "        num, val, _ = p.split()\n",
    "        num = int(num)\n",
    "        if val.startswith('hour'):\n",
    "            upload_dt = today - td(hours=num)\n",
    "        elif val.startswith('day'):\n",
    "            upload_dt = today - td(days=num)\n",
    "        elif val.startswith('week'):\n",
    "            upload_dt = today - td(weeks=num) \n",
    "        elif val.startswith('month'):\n",
    "            upload_dt = today - td(days=num*30) \n",
    "        elif val.startswith('year'):\n",
    "            upload_dt = today - td(days=num*365) \n",
    "\n",
    "        date_lst.append(upload_dt.strftime(\"%d %b %Y\"))\n",
    "        \n",
    "    return date_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "working-cookbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03 Mar 2023', '03 Mar 2023', '02 Mar 2023', '02 Mar 2023', '02 Mar 2023']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5publish_dates = yt_get_video_publish_date(top5post)\n",
    "top5publish_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-atlanta",
   "metadata": {},
   "source": [
    "#### Saving all above data in CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "assigned-xerox",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_title</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>total_views</th>\n",
       "      <th>video_url</th>\n",
       "      <th>thumbnail_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Complete ğ—”ğ—–ğ—œğ——, ğ—•ğ—”ğ—¦ğ—˜ğ—¦ ğ—”ğ—¡ğ—— ğ—¦ğ—”ğ—Ÿğ—§ in 110 Minutes |...</td>\n",
       "      <td>03 Mar 2023</td>\n",
       "      <td>4,866 views</td>\n",
       "      <td>https://www.youtube.com/watch?v=46CNHP9wBAs</td>\n",
       "      <td>https://i.ytimg.com/vi/46CNHP9wBAs/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Complete ğ—–ğ—›ğ—˜ğ— ğ—œğ—–ğ—”ğ—Ÿ ğ—¥ğ—˜ğ—”ğ—–ğ—§ğ—œğ—¢ğ—¡  in 90 Minutes | Cl...</td>\n",
       "      <td>03 Mar 2023</td>\n",
       "      <td>14,646 views</td>\n",
       "      <td>https://www.youtube.com/watch?v=1Wk5gJtZ2sQ</td>\n",
       "      <td>https://i.ytimg.com/vi/1Wk5gJtZ2sQ/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Complete ğ‡ğ„ğ‘ğ„ğƒğˆğ“ğ˜ ğ€ğğƒ ğ„ğ•ğğ‹ğ”ğ“ğˆğğ in 2 Hours|| C...</td>\n",
       "      <td>02 Mar 2023</td>\n",
       "      <td>38,616 views</td>\n",
       "      <td>https://www.youtube.com/watch?v=iM_hVnElC-Q</td>\n",
       "      <td>https://i.ytimg.com/vi/iM_hVnElC-Q/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Complete ğ—›ğ—¢ğ—ª ğ——ğ—¢ ğ—¢ğ—¥ğ—šğ—”ğ—¡ğ—œğ—¦ğ— ğ—¦ ğ—¥ğ—˜ğ—£ğ—¥ğ—¢ğ——ğ—¨ğ—–ğ—˜ in 1 Hours...</td>\n",
       "      <td>02 Mar 2023</td>\n",
       "      <td>17,563 views</td>\n",
       "      <td>https://www.youtube.com/watch?v=Prly9d7LoAQ</td>\n",
       "      <td>https://i.ytimg.com/vi/Prly9d7LoAQ/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Complete ğ—¢ğ—¨ğ—¥ ğ—˜ğ—¡ğ—©ğ—œğ—¥ğ—¢ğ—¡ğ— ğ—˜ğ—¡ğ—§ in 1 Hour 30 Minutes ...</td>\n",
       "      <td>02 Mar 2023</td>\n",
       "      <td>32,307 views</td>\n",
       "      <td>https://www.youtube.com/watch?v=AvwBDiCWSEM</td>\n",
       "      <td>https://i.ytimg.com/vi/AvwBDiCWSEM/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_title publish_date  \\\n",
       "0  Complete ğ—”ğ—–ğ—œğ——, ğ—•ğ—”ğ—¦ğ—˜ğ—¦ ğ—”ğ—¡ğ—— ğ—¦ğ—”ğ—Ÿğ—§ in 110 Minutes |...  03 Mar 2023   \n",
       "1  Complete ğ—–ğ—›ğ—˜ğ— ğ—œğ—–ğ—”ğ—Ÿ ğ—¥ğ—˜ğ—”ğ—–ğ—§ğ—œğ—¢ğ—¡  in 90 Minutes | Cl...  03 Mar 2023   \n",
       "2  Complete ğ‡ğ„ğ‘ğ„ğƒğˆğ“ğ˜ ğ€ğğƒ ğ„ğ•ğğ‹ğ”ğ“ğˆğğ in 2 Hours|| C...  02 Mar 2023   \n",
       "3  Complete ğ—›ğ—¢ğ—ª ğ——ğ—¢ ğ—¢ğ—¥ğ—šğ—”ğ—¡ğ—œğ—¦ğ— ğ—¦ ğ—¥ğ—˜ğ—£ğ—¥ğ—¢ğ——ğ—¨ğ—–ğ—˜ in 1 Hours...  02 Mar 2023   \n",
       "4  Complete ğ—¢ğ—¨ğ—¥ ğ—˜ğ—¡ğ—©ğ—œğ—¥ğ—¢ğ—¡ğ— ğ—˜ğ—¡ğ—§ in 1 Hour 30 Minutes ...  02 Mar 2023   \n",
       "\n",
       "    total_views                                    video_url  \\\n",
       "0   4,866 views  https://www.youtube.com/watch?v=46CNHP9wBAs   \n",
       "1  14,646 views  https://www.youtube.com/watch?v=1Wk5gJtZ2sQ   \n",
       "2  38,616 views  https://www.youtube.com/watch?v=iM_hVnElC-Q   \n",
       "3  17,563 views  https://www.youtube.com/watch?v=Prly9d7LoAQ   \n",
       "4  32,307 views  https://www.youtube.com/watch?v=AvwBDiCWSEM   \n",
       "\n",
       "                                      thumbnail_url  \n",
       "0  https://i.ytimg.com/vi/46CNHP9wBAs/hqdefault.jpg  \n",
       "1  https://i.ytimg.com/vi/1Wk5gJtZ2sQ/hqdefault.jpg  \n",
       "2  https://i.ytimg.com/vi/iM_hVnElC-Q/hqdefault.jpg  \n",
       "3  https://i.ytimg.com/vi/Prly9d7LoAQ/hqdefault.jpg  \n",
       "4  https://i.ytimg.com/vi/AvwBDiCWSEM/hqdefault.jpg  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pwskills = {'video_title': top5titles, \n",
    "            'publish_date': top5publish_dates, \n",
    "            'total_views': top5views,\n",
    "            'video_url': top5urls,\n",
    "            'thumbnail_url': top5thumbnails}\n",
    "\n",
    "yt_df = pd.DataFrame(pwskills)\n",
    "yt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "selective-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the yt_df data in csv file\n",
    "yt_df.to_csv('PWSkillsChannelVideos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-transportation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
