{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "voluntary-discretion",
   "metadata": {},
   "source": [
    "## <u> Assignment 20 - 21st Feb</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-bread",
   "metadata": {},
   "source": [
    "#### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "\n",
    "**Web Scraping** is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications.\n",
    "\n",
    "Web scraping, or web content extraction, can serve an unlimited number of purposes.\n",
    "Whether you're a new business or a growing one, web scraping helps you 10x your business growth with web data.\n",
    "\n",
    "Here are some of the reasons why we should use web scrapping.\n",
    "\n",
    "- Technology makes it easy to extract data\n",
    "- Innovation at the speed of light\n",
    "- Better access to company data\n",
    "- Lead generation to build a sales machine\n",
    "- Marketing automation without limits\n",
    "- Brand monitoring for everyone\n",
    "- Market analysis at scale\n",
    "\n",
    "Some of the widely used areas for web scraping are:\n",
    "\n",
    "\n",
    "- **Price Monitoring**\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "\n",
    "- **Sentiment Analysis**\n",
    "If companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition\n",
    "\n",
    "\n",
    "- **Email Marketing**\n",
    "Companies can also use Web scraping for email marketing. They can collect Email IDâ€™s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID's.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-turkish",
   "metadata": {},
   "source": [
    "#### Q2. What are the different methods used for Web Scraping?\n",
    "\n",
    "Here are some to the methods/techniques used for web scarping:\n",
    "\n",
    "**Human copy-and-pase**\n",
    "\n",
    "The simplest form of web scraping is manually copying and pasting data from the web page into text file or spreadsheet.\n",
    "Sometimes even the best web-scraping technology cannot replace a human's manual examination and copy-and-paste, and sometimes this may be the only workable solution when the websites for scraping explicitly set up barriers to prevent machine automation.\n",
    "\n",
    "\n",
    "**Text pattern matching**\n",
    "\n",
    "A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages (for instance Perl or Python).\n",
    "\n",
    "\n",
    "**HTTP programming**\n",
    "\n",
    "Static and dynamic web pages can be retrieved by posting HTTP requests to the remote web server using socket programming.\n",
    "\n",
    "\n",
    "**HTML parsing**\n",
    "\n",
    "Many websites have large collections of pages generated dynamically from an underlying structured source like a database. Data of the same category are typically encoded into similar pages by a common script or template. In data mining, a program that detects such templates in a particular information source, extracts its content and translates it into a relational form, is called a wrapper. Some popular tools for HTML parsing in python are BeautifulSoup, Scrapy, and Selenium.\n",
    "\n",
    "\n",
    "**DOM parsing**\n",
    "\n",
    "**DOM** is stands for **D**ocument **O**bject **M**odel.\n",
    "By embedding a full-fledged web browser, such as the Internet Explorer or the Mozilla browser control, programs can retrieve the dynamic content generated by client-side scripts. These browser controls also parse web pages into a DOM tree, based on which programs can retrieve parts of the pages. Languages such as Xpath can be used to parse the resulting DOM tree.\n",
    "\n",
    "\n",
    "**Computer vision web-page analysis**\n",
    "\n",
    "There are efforts using machine learning and computer vision that attempt to identify and extract information from web pages by interpreting pages visually as a human being might.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-candy",
   "metadata": {},
   "source": [
    "#### Q3. What is Beautiful Soup? Why is it used?\n",
    "\n",
    "Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files. It transforms a complex HTML document into a tree of Python objects. It also automatically converts the document to Unicode, so you don't have to think about encodings. This tool not only helps you scrape but also to clean the data. Beautiful Soup supports the HTML parser included in Python's standard library, but it also supports several third-party Python parsers like lxml or hml5lib.\n",
    "\n",
    "Beautiful Soup is used for web scraping because of following reason:\n",
    "\n",
    "- It supports multiple parsing methods like HTML and XML\n",
    "- It has ability to extract data even from unstructured HTML page\n",
    "- It is easy to install and learn for beginners\n",
    "- It has a vast community of developers who contributed and provide support\n",
    "- It is reliable and stable library for web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-gamma",
   "metadata": {},
   "source": [
    "#### Q4. Why is flask used in this Web Scraping project?\n",
    "\n",
    "Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries.\n",
    "\n",
    "For web scraping project, if you want to create user interface which will ask user to enter some URLs or simply search some product/query, you need to create some HTML page and Flask is used for it. Let's take an example that you want to collect data from Amazon website for different different product and store them in text/db, you need to create simple UI for user to enter product/item name and your web scraper collect the information such as product's names, prices, ratings, comments, etc. from Amazon website.\n",
    "\n",
    "\n",
    "After that you can share the results of store data with others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-clinic",
   "metadata": {},
   "source": [
    "#### Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "The two services which are used in this project are as follows:\n",
    "\n",
    "- **Elastic Beanstalk**\n",
    "\n",
    "AWS Elastic Beanstalk is a fully managed service that makes it easy for us to deploy, manage, and scale our applications and services. Elastic Beanstalk provides platforms for programming languages like Go, Java, Node.js, PHP, Python, Ruby,  application servers like Tomcat, Passenger, Puma, and Docker containers. Some platforms have multiple concurrently-supported versions.\n",
    "\n",
    "Elastic Beanstalk provisions the resources needed to run your application, including one or more Amazon EC2 instances. The software stack running on the Amazon EC2 instances depends on the specific platform version you've selected for your environment.\n",
    "\n",
    "We can also customize the environment that Elastic Beanstalk creates for our application, including the instance type, operating system, and database configuration. We can also integrate our application with other AWS services, such as Amazon RDS for databases, Amazon S3 for storage, and Amazon CloudWatch for monitoring.\n",
    "\n",
    "It offers a range of deployment options, including rolling updates, blue/green deployments, and canary deployments. This enables us to choose the deployment method that best suits our application and business needs.\n",
    "\n",
    "- **Code Pipeline**\n",
    "\n",
    "AWS CodePipeline is a fully managed continuous delivery service.With the help of CodePipeline, we can build, test, stage, and deploy our code whenever there is modification in our code, based on the release model.\n",
    "\n",
    "CodePipeline allows us to create a pipeline that consists of a series of stages, each of which represents a step in our software release process. Each stage can have one or more actions, such as building our code, running tests, and deploying our code to a production environment.\n",
    "\n",
    "We can use CodePipeline with third-party tools and services, like we use it with GitHub in our example, to customize our pipeline and incorporate our existing workflows.\n",
    "\n",
    "By using CodePipeline, we can increase the speed and reliability of our software release process, reduce manual errors, and improve collaboration between development and operations teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-blues",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
